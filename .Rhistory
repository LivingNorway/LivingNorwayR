}
}
# Function to find the classes contained in the current URL
findGBIFClasses <- function(specAddress, termList) {
# Function to generate a GBIF class from a list of terms
createGBIFClass <- function(specAddress, termList) {
# Function to add a trailing "/" if there isn't one
addTrailingDir <- function(inText) {
outText <- as.character(inText)
if(!grepl("\\/$", outText, perl = TRUE)) {
outText <- paste(outText, "/", sep = "")
}
outText
}
curDoc <- read_xml(specAddress)
# Get the qualified names of each of the members of the class
memberTerms <- sapply(X = xml_children(curDoc), FUN = function(curNode) {
paste(addTrailingDir(xml_attr(curNode, "namespace")), xml_attr(curNode, "name"), sep = "")
})
memberTerms <- memberTerms[memberTerms %in% names(termList)]
# Format the output object into a list of terms for the class
list(
termInfo = termList[[paste(addTrailingDir(xml_attr(curDoc, "namespace")), xml_attr(curDoc, "name"), sep = "")]],
compositeTerms = termList[memberTerms]
)
}
# Retrieve the link nodes for all the entries in the specification page
if(!grepl("/$", specAddress, perl = TRUE)) {
specAddress <- paste(specAddress, "/", sep = "")
}
aNodes <- xml_find_all(read_html(specAddress), "//td/a")
aLinks <- sapply(X = aNodes[2:length(aNodes)], FUN = xml_attr, attr = "href")
# Retrieve the links that are directories
dirLinks <- aLinks[grepl("/$", aLinks, perl = TRUE)]
# For the links that are XML files: make sure the links are only the most recently defined
xmlLinks <- aLinks[grepl("\\.xml$", aLinks, perl = TRUE)]
xmlLinks <- sapply(X = unique(gsub("_*\\d\\d\\d\\d[_-]\\d\\d[_-]\\d\\d_*", "", xmlLinks, perl = TRUE)), FUN = function(curLink, xmlLinks) {
possLinks <- xmlLinks[curLink == gsub("_*\\d\\d\\d\\d[_-]\\d\\d[_-]\\d\\d_*", "", xmlLinks, perl = TRUE)]
if(length(possLinks) > 1) {
# If the there are multiple possible links for the class definition then use the most recent definition
curDates <- strptime(
gsub("^.*(\\d\\d\\d\\d)[_-](\\d\\d)[_-](\\d\\d).*$", "\\1-\\2-\\3", possLinks, perl = TRUE),
"%Y-%m-%d")
possLinks <- possLinks[which.max(ifelse(is.na(curDates), -Inf, as.double(as.POSIXlt(curDates))))]
}
if(length(possLinks) == 0) {
possLinks <- curLink
}
possLinks
}, xmlLinks = xmlLinks)
if(names(xmlLinks) == "amplification.xml") {
browser()
}
# Go through each of the links and process each entry
do.call(c, lapply(X = c(dirLinks, xmlLinks), FUN = function(curLink, curBaseAddress, termList) {
outVals <- list()
if(grepl("\\.xml$", curLink, perl = TRUE)) {
# If the link is to a XML file then scrape the property information from it
outVals <- list(createGBIFClass(paste(curBaseAddress, curLink, sep = ""), termList))
} else if(grepl("/$", curLink, perl = TRUE)) {
# If the link is to another folder then call the function recursively
outVals <- findGBIFClasses(paste(curBaseAddress, curLink, sep = ""), termList)
}
outVals
}, curBaseAddress = specAddress, termList = termList))
}
outList <- do.call(c, lapply(X = urlCheck, FUN = findGBIFClasses, termList = termList))
outList <- outList[sapply(X = outList, FUN = function(curEl) {!is.null(curEl$termInfo)})]
names(outList) <- sapply(X = outList, FUN = function(curEl) {curEl$termInfo$getQualifiedName()})
outList
}
# Retrieve all the GBIF classes and their members
allGBIFClassList <- retrieveGBIFClassSpecifications("all", TRUE)
retrieveGBIFClassSpecifications <- function(classOption = "all", includeDeprecated = FALSE) {
# ====== 4.1. Retrieve terms from the GBIF list of used terms ======
termList <- retrieveDwCTermSpecifications(TRUE, includeDeprecated)
# ====== 4.2. Assign terms to their respective GBIF classes ======
# Sanity test the class option input
inClassOption <- tryCatch(tolower(as.character(classOption)), error = function(err) {
stop("error encountered processing the class option parameter: ", err)
})
if(length(inClassOption) > 1) {
warning("class option parameter length greater than one: only the first element will be used")
inClassOption <- inClassOption[1]
} else if(length(inClassOption) == 0) {
stop("error encountered processing the class option parameter: parameter has length 0")
}
# The URLs to check the class structure from
urlCheck <- c("https://rs.gbif.org/core/", "https://rs.gbif.org/extension/")
if(inClassOption != "all") {
if(inClassOption == "core") {
urlCheck <- urlCheck[1]
} else if(inClassOption == "extension") {
urlCheck <- urlCheck[2]
} else {
stop("error encountered processing the class option parameter: values must be \"all\", \"core\", or \"extension\"")
}
}
# Function to find the classes contained in the current URL
findGBIFClasses <- function(specAddress, termList) {
# Function to generate a GBIF class from a list of terms
createGBIFClass <- function(specAddress, termList) {
# Function to add a trailing "/" if there isn't one
addTrailingDir <- function(inText) {
outText <- as.character(inText)
if(!grepl("\\/$", outText, perl = TRUE)) {
outText <- paste(outText, "/", sep = "")
}
outText
}
curDoc <- read_xml(specAddress)
# Get the qualified names of each of the members of the class
memberTerms <- sapply(X = xml_children(curDoc), FUN = function(curNode) {
paste(addTrailingDir(xml_attr(curNode, "namespace")), xml_attr(curNode, "name"), sep = "")
})
memberTerms <- memberTerms[memberTerms %in% names(termList)]
# Format the output object into a list of terms for the class
list(
termInfo = termList[[paste(addTrailingDir(xml_attr(curDoc, "namespace")), xml_attr(curDoc, "name"), sep = "")]],
compositeTerms = termList[memberTerms]
)
}
# Retrieve the link nodes for all the entries in the specification page
if(!grepl("/$", specAddress, perl = TRUE)) {
specAddress <- paste(specAddress, "/", sep = "")
}
aNodes <- xml_find_all(read_html(specAddress), "//td/a")
aLinks <- sapply(X = aNodes[2:length(aNodes)], FUN = xml_attr, attr = "href")
# Retrieve the links that are directories
dirLinks <- aLinks[grepl("/$", aLinks, perl = TRUE)]
# For the links that are XML files: make sure the links are only the most recently defined
xmlLinks <- aLinks[grepl("\\.xml$", aLinks, perl = TRUE)]
xmlLinks <- sapply(X = unique(gsub("_*\\d\\d\\d\\d[_-]\\d\\d[_-]\\d\\d_*", "", xmlLinks, perl = TRUE)), FUN = function(curLink, xmlLinks) {
possLinks <- xmlLinks[curLink == gsub("_*\\d\\d\\d\\d[_-]\\d\\d[_-]\\d\\d_*", "", xmlLinks, perl = TRUE)]
if(length(possLinks) > 1) {
# If the there are multiple possible links for the class definition then use the most recent definition
curDates <- strptime(
gsub("^.*(\\d\\d\\d\\d)[_-](\\d\\d)[_-](\\d\\d).*$", "\\1-\\2-\\3", possLinks, perl = TRUE),
"%Y-%m-%d")
possLinks <- possLinks[which.max(ifelse(is.na(curDates), -Inf, as.double(as.POSIXlt(curDates))))]
}
if(length(possLinks) == 0) {
possLinks <- curLink
}
possLinks
}, xmlLinks = xmlLinks)
if(length(xmlLinks) > 0 && any(names(xmlLinks) == "amplification.xml")) {
browser()
}
# Go through each of the links and process each entry
do.call(c, lapply(X = c(dirLinks, xmlLinks), FUN = function(curLink, curBaseAddress, termList) {
outVals <- list()
if(grepl("\\.xml$", curLink, perl = TRUE)) {
# If the link is to a XML file then scrape the property information from it
outVals <- list(createGBIFClass(paste(curBaseAddress, curLink, sep = ""), termList))
} else if(grepl("/$", curLink, perl = TRUE)) {
# If the link is to another folder then call the function recursively
outVals <- findGBIFClasses(paste(curBaseAddress, curLink, sep = ""), termList)
}
outVals
}, curBaseAddress = specAddress, termList = termList))
}
outList <- do.call(c, lapply(X = urlCheck, FUN = findGBIFClasses, termList = termList))
outList <- outList[sapply(X = outList, FUN = function(curEl) {!is.null(curEl$termInfo)})]
names(outList) <- sapply(X = outList, FUN = function(curEl) {curEl$termInfo$getQualifiedName()})
outList
}
# Retrieve all the GBIF classes and their members
allGBIFClassList <- retrieveGBIFClassSpecifications("all", TRUE)
xmlLinks
# Retrieve all the GBIF classes and their members
allGBIFClassList <- retrieveGBIFClassSpecifications("all", TRUE)
names(allGBIFClassList)
# Retrieve all the GBIF classes and their members
allGBIFClassList <- retrieveGBIFClassSpecifications("all", TRUE)
dirLinks
do.call(c, lapply(X = c(dirLinks, xmlLinks), FUN = function(curLink, curBaseAddress, termList) {
outVals <- list()
if(grepl("\\.xml$", curLink, perl = TRUE)) {
# If the link is to a XML file then scrape the property information from it
outVals <- list(createGBIFClass(paste(curBaseAddress, curLink, sep = ""), termList))
} else if(grepl("/$", curLink, perl = TRUE)) {
# If the link is to another folder then call the function recursively
outVals <- findGBIFClasses(paste(curBaseAddress, curLink, sep = ""), termList)
}
outVals
}, curBaseAddress = specAddress, termList = termList))
retrieveGBIFClassSpecifications <- function(classOption = "all", includeDeprecated = FALSE) {
# ====== 4.1. Retrieve terms from the GBIF list of used terms ======
termList <- retrieveDwCTermSpecifications(TRUE, includeDeprecated)
# ====== 4.2. Assign terms to their respective GBIF classes ======
# Sanity test the class option input
inClassOption <- tryCatch(tolower(as.character(classOption)), error = function(err) {
stop("error encountered processing the class option parameter: ", err)
})
if(length(inClassOption) > 1) {
warning("class option parameter length greater than one: only the first element will be used")
inClassOption <- inClassOption[1]
} else if(length(inClassOption) == 0) {
stop("error encountered processing the class option parameter: parameter has length 0")
}
# The URLs to check the class structure from
urlCheck <- c("https://rs.gbif.org/core/", "https://rs.gbif.org/extension/")
if(inClassOption != "all") {
if(inClassOption == "core") {
urlCheck <- urlCheck[1]
} else if(inClassOption == "extension") {
urlCheck <- urlCheck[2]
} else {
stop("error encountered processing the class option parameter: values must be \"all\", \"core\", or \"extension\"")
}
}
# Function to find the classes contained in the current URL
findGBIFClasses <- function(specAddress, termList) {
# Function to generate a GBIF class from a list of terms
createGBIFClass <- function(specAddress, termList) {
# Function to add a trailing "/" if there isn't one
addTrailingDir <- function(inText) {
outText <- as.character(inText)
if(!grepl("\\/$", outText, perl = TRUE)) {
outText <- paste(outText, "/", sep = "")
}
outText
}
curDoc <- read_xml(specAddress)
# Get the qualified names of each of the members of the class
memberTerms <- sapply(X = xml_children(curDoc), FUN = function(curNode) {
paste(addTrailingDir(xml_attr(curNode, "namespace")), xml_attr(curNode, "name"), sep = "")
})
memberTerms <- memberTerms[memberTerms %in% names(termList)]
# Format the output object into a list of terms for the class
list(
termInfo = termList[[paste(addTrailingDir(xml_attr(curDoc, "namespace")), xml_attr(curDoc, "name"), sep = "")]],
compositeTerms = termList[memberTerms]
)
}
# Retrieve the link nodes for all the entries in the specification page
if(!grepl("/$", specAddress, perl = TRUE)) {
specAddress <- paste(specAddress, "/", sep = "")
}
aNodes <- xml_find_all(read_html(specAddress), "//td/a")
aLinks <- sapply(X = aNodes[2:length(aNodes)], FUN = xml_attr, attr = "href")
# Retrieve the links that are directories
dirLinks <- aLinks[grepl("/$", aLinks, perl = TRUE)]
# For the links that are XML files: make sure the links are only the most recently defined
xmlLinks <- aLinks[grepl("\\.xml$", aLinks, perl = TRUE)]
xmlLinks <- sapply(X = unique(gsub("_*\\d\\d\\d\\d[_-]\\d\\d[_-]\\d\\d_*", "", xmlLinks, perl = TRUE)), FUN = function(curLink, xmlLinks) {
possLinks <- xmlLinks[curLink == gsub("_*\\d\\d\\d\\d[_-]\\d\\d[_-]\\d\\d_*", "", xmlLinks, perl = TRUE)]
if(length(possLinks) > 1) {
# If the there are multiple possible links for the class definition then use the most recent definition
curDates <- strptime(
gsub("^.*(\\d\\d\\d\\d)[_-](\\d\\d)[_-](\\d\\d).*$", "\\1-\\2-\\3", possLinks, perl = TRUE),
"%Y-%m-%d")
possLinks <- possLinks[which.max(ifelse(is.na(curDates), -Inf, as.double(as.POSIXlt(curDates))))]
}
if(length(possLinks) == 0) {
possLinks <- curLink
}
possLinks
}, xmlLinks = xmlLinks)
# Go through each of the links and process each entry
do.call(c, lapply(X = c(dirLinks, xmlLinks), FUN = function(curLink, curBaseAddress, termList) {
outVals <- list()
if(grepl("\\.xml$", curLink, perl = TRUE)) {
# If the link is to a XML file then scrape the property information from it
outVals <- list(createGBIFClass(paste(curBaseAddress, curLink, sep = ""), termList))
} else if(grepl("/$", curLink, perl = TRUE)) {
# If the link is to another folder then call the function recursively
outVals <- findGBIFClasses(paste(curBaseAddress, curLink, sep = ""), termList)
}
outVals
}, curBaseAddress = specAddress, termList = termList))
}
browser()
outList <- do.call(c, lapply(X = urlCheck, FUN = findGBIFClasses, termList = termList))
outList <- outList[sapply(X = outList, FUN = function(curEl) {!is.null(curEl$termInfo)})]
names(outList) <- sapply(X = outList, FUN = function(curEl) {curEl$termInfo$getQualifiedName()})
outList
}
# Retrieve all the GBIF classes and their members
allGBIFClassList <- retrieveGBIFClassSpecifications("all", TRUE)
# Retrieve all the GBIF classes and their members
allGBIFClassList <- retrieveGBIFClassSpecifications("all", TRUE)
retrieveGBIFClassSpecifications <- function(classOption = "all", includeDeprecated = FALSE) {
# ====== 4.1. Retrieve terms from the GBIF list of used terms ======
termList <- retrieveDwCTermSpecifications(TRUE, includeDeprecated)
# ====== 4.2. Assign terms to their respective GBIF classes ======
# Sanity test the class option input
inClassOption <- tryCatch(tolower(as.character(classOption)), error = function(err) {
stop("error encountered processing the class option parameter: ", err)
})
if(length(inClassOption) > 1) {
warning("class option parameter length greater than one: only the first element will be used")
inClassOption <- inClassOption[1]
} else if(length(inClassOption) == 0) {
stop("error encountered processing the class option parameter: parameter has length 0")
}
# The URLs to check the class structure from
urlCheck <- c("https://rs.gbif.org/core/", "https://rs.gbif.org/extension/")
if(inClassOption != "all") {
if(inClassOption == "core") {
urlCheck <- urlCheck[1]
} else if(inClassOption == "extension") {
urlCheck <- urlCheck[2]
} else {
stop("error encountered processing the class option parameter: values must be \"all\", \"core\", or \"extension\"")
}
}
# Function to find the classes contained in the current URL
findGBIFClasses <- function(specAddress, termList) {
# Function to generate a GBIF class from a list of terms
createGBIFClass <- function(specAddress, termList) {
# Function to add a trailing "/" if there isn't one
addTrailingDir <- function(inText) {
outText <- as.character(inText)
if(!grepl("\\/$", outText, perl = TRUE)) {
outText <- paste(outText, "/", sep = "")
}
outText
}
curDoc <- read_xml(specAddress)
# Get the qualified names of each of the members of the class
memberTerms <- sapply(X = xml_children(curDoc), FUN = function(curNode) {
paste(addTrailingDir(xml_attr(curNode, "namespace")), xml_attr(curNode, "name"), sep = "")
})
memberTerms <- memberTerms[memberTerms %in% names(termList)]
# Format the output object into a list of terms for the class
list(
termInfo = termList[[paste(addTrailingDir(xml_attr(curDoc, "namespace")), xml_attr(curDoc, "name"), sep = "")]],
compositeTerms = termList[memberTerms]
)
}
# Retrieve the link nodes for all the entries in the specification page
if(!grepl("/$", specAddress, perl = TRUE)) {
specAddress <- paste(specAddress, "/", sep = "")
}
aNodes <- xml_find_all(read_html(specAddress), "//td/a")
aLinks <- sapply(X = aNodes[2:length(aNodes)], FUN = xml_attr, attr = "href")
# Retrieve the links that are directories
dirLinks <- aLinks[grepl("/$", aLinks, perl = TRUE)]
# For the links that are XML files: make sure the links are only the most recently defined
xmlLinks <- aLinks[grepl("\\.xml$", aLinks, perl = TRUE)]
xmlLinks <- sapply(X = unique(gsub("_*\\d\\d\\d\\d[_-]\\d\\d[_-]\\d\\d_*", "", xmlLinks, perl = TRUE)), FUN = function(curLink, xmlLinks) {
possLinks <- xmlLinks[curLink == gsub("_*\\d\\d\\d\\d[_-]\\d\\d[_-]\\d\\d_*", "", xmlLinks, perl = TRUE)]
if(length(possLinks) > 1) {
# If the there are multiple possible links for the class definition then use the most recent definition
curDates <- strptime(
gsub("^.*(\\d\\d\\d\\d)[_-](\\d\\d)[_-](\\d\\d).*$", "\\1-\\2-\\3", possLinks, perl = TRUE),
"%Y-%m-%d")
possLinks <- possLinks[which.max(ifelse(is.na(curDates), -Inf, as.double(as.POSIXlt(curDates))))]
}
if(length(possLinks) == 0) {
possLinks <- curLink
}
possLinks
}, xmlLinks = xmlLinks)
# Go through each of the links and process each entry
do.call(c, lapply(X = c(dirLinks, xmlLinks), FUN = function(curLink, curBaseAddress, termList) {
outVals <- list()
if(grepl("\\.xml$", curLink, perl = TRUE)) {
# If the link is to a XML file then scrape the property information from it
outVals <- list(createGBIFClass(paste(curBaseAddress, curLink, sep = ""), termList))
} else if(grepl("/$", curLink, perl = TRUE)) {
# If the link is to another folder then call the function recursively
outVals <- findGBIFClasses(paste(curBaseAddress, curLink, sep = ""), termList)
}
outVals
}, curBaseAddress = specAddress, termList = termList))
}
outList <- do.call(c, lapply(X = urlCheck, FUN = findGBIFClasses, termList = termList))
browser()
outList <- outList[sapply(X = outList, FUN = function(curEl) {!is.null(curEl$termInfo)})]
names(outList) <- sapply(X = outList, FUN = function(curEl) {curEl$termInfo$getQualifiedName()})
outList
}
# Retrieve all the GBIF classes and their members
allGBIFClassList <- retrieveGBIFClassSpecifications("all", TRUE)
outList
sapply(X = outList, FUN = function(curEl) {curEl$termInfo$getQualifiedName()})
retrieveGBIFClassSpecifications <- function(classOption = "all", includeDeprecated = FALSE) {
# ====== 4.1. Retrieve terms from the GBIF list of used terms ======
termList <- retrieveDwCTermSpecifications(TRUE, includeDeprecated)
# ====== 4.2. Assign terms to their respective GBIF classes ======
# Sanity test the class option input
inClassOption <- tryCatch(tolower(as.character(classOption)), error = function(err) {
stop("error encountered processing the class option parameter: ", err)
})
if(length(inClassOption) > 1) {
warning("class option parameter length greater than one: only the first element will be used")
inClassOption <- inClassOption[1]
} else if(length(inClassOption) == 0) {
stop("error encountered processing the class option parameter: parameter has length 0")
}
# The URLs to check the class structure from
urlCheck <- c("https://rs.gbif.org/core/", "https://rs.gbif.org/extension/")
if(inClassOption != "all") {
if(inClassOption == "core") {
urlCheck <- urlCheck[1]
} else if(inClassOption == "extension") {
urlCheck <- urlCheck[2]
} else {
stop("error encountered processing the class option parameter: values must be \"all\", \"core\", or \"extension\"")
}
}
# Function to find the classes contained in the current URL
findGBIFClasses <- function(specAddress, termList) {
# Function to generate a GBIF class from a list of terms
createGBIFClass <- function(specAddress, termList) {
# Function to add a trailing "/" if there isn't one
addTrailingDir <- function(inText) {
outText <- as.character(inText)
if(!grepl("\\/$", outText, perl = TRUE)) {
outText <- paste(outText, "/", sep = "")
}
outText
}
curDoc <- read_xml(specAddress)
# Get the qualified names of each of the members of the class
memberTerms <- sapply(X = xml_children(curDoc), FUN = function(curNode) {
paste(addTrailingDir(xml_attr(curNode, "namespace")), xml_attr(curNode, "name"), sep = "")
})
memberTerms <- memberTerms[memberTerms %in% names(termList)]
# Format the output object into a list of terms for the class
list(
termInfo = termList[[paste(addTrailingDir(xml_attr(curDoc, "namespace")), xml_attr(curDoc, "name"), sep = "")]],
compositeTerms = termList[memberTerms]
)
}
# Retrieve the link nodes for all the entries in the specification page
if(!grepl("/$", specAddress, perl = TRUE)) {
specAddress <- paste(specAddress, "/", sep = "")
}
aNodes <- xml_find_all(read_html(specAddress), "//td/a")
aLinks <- sapply(X = aNodes[2:length(aNodes)], FUN = xml_attr, attr = "href")
# Retrieve the links that are directories
dirLinks <- aLinks[grepl("/$", aLinks, perl = TRUE)]
# For the links that are XML files: make sure the links are only the most recently defined
xmlLinks <- aLinks[grepl("\\.xml$", aLinks, perl = TRUE)]
xmlLinks <- sapply(X = unique(gsub("_*\\d\\d\\d\\d[_-]\\d\\d[_-]\\d\\d_*", "", xmlLinks, perl = TRUE)), FUN = function(curLink, xmlLinks) {
possLinks <- xmlLinks[curLink == gsub("_*\\d\\d\\d\\d[_-]\\d\\d[_-]\\d\\d_*", "", xmlLinks, perl = TRUE)]
if(length(possLinks) > 1) {
# If the there are multiple possible links for the class definition then use the most recent definition
curDates <- strptime(
gsub("^.*(\\d\\d\\d\\d)[_-](\\d\\d)[_-](\\d\\d).*$", "\\1-\\2-\\3", possLinks, perl = TRUE),
"%Y-%m-%d")
possLinks <- possLinks[which.max(ifelse(is.na(curDates), -Inf, as.double(as.POSIXlt(curDates))))]
}
if(length(possLinks) == 0) {
possLinks <- curLink
}
possLinks
}, xmlLinks = xmlLinks)
if(length(xmlLinks) > 0 && any(xmlLinks == "amplification.xml")) {
browser()
}
# Go through each of the links and process each entry
do.call(c, lapply(X = c(dirLinks, xmlLinks), FUN = function(curLink, curBaseAddress, termList) {
outVals <- list()
if(grepl("\\.xml$", curLink, perl = TRUE)) {
# If the link is to a XML file then scrape the property information from it
outVals <- list(createGBIFClass(paste(curBaseAddress, curLink, sep = ""), termList))
} else if(grepl("/$", curLink, perl = TRUE)) {
# If the link is to another folder then call the function recursively
outVals <- findGBIFClasses(paste(curBaseAddress, curLink, sep = ""), termList)
}
outVals
}, curBaseAddress = specAddress, termList = termList))
}
outList <- do.call(c, lapply(X = urlCheck, FUN = findGBIFClasses, termList = termList))
browser()
outList <- outList[sapply(X = outList, FUN = function(curEl) {!is.null(curEl$termInfo)})]
names(outList) <- sapply(X = outList, FUN = function(curEl) {curEl$termInfo$getQualifiedName()})
outList
}
# Retrieve all the GBIF classes and their members
allGBIFClassList <- retrieveGBIFClassSpecifications("all", TRUE)
xmlLinks
testOut <- do.call(c, lapply(X = c(dirLinks, xmlLinks), FUN = function(curLink, curBaseAddress, termList) {
outVals <- list()
if(grepl("\\.xml$", curLink, perl = TRUE)) {
# If the link is to a XML file then scrape the property information from it
outVals <- list(createGBIFClass(paste(curBaseAddress, curLink, sep = ""), termList))
} else if(grepl("/$", curLink, perl = TRUE)) {
# If the link is to another folder then call the function recursively
outVals <- findGBIFClasses(paste(curBaseAddress, curLink, sep = ""), termList)
}
outVals
}, curBaseAddress = specAddress, termList = termList))
testOut
lapply(X = testOut, FUN = function(curEl) { curEl$termInfo })
curDoc <- read_xml("https://rs.gbif.org/extension/ggbn/cloning.xml")
curDoc
xml_attr(curDoc, "relation")
xml_attrs(curDoc)
xml_attr(curDoc, "rowType")
source('~/Work/LivingNorway/LivingNorwayR_GitHub/R/DwCTerm.R', echo=TRUE)
library(R6)
library(xml2)
library(zip)
source('~/Work/LivingNorway/LivingNorwayR_GitHub/R/DwCTerm.R', echo=TRUE)
source('~/Work/LivingNorway/LivingNorwayR_GitHub/data-raw/DwCTermList.R', echo=TRUE)
names(DwCTermList)
source('~/Work/LivingNorway/LivingNorwayR_GitHub/data-raw/GBIFClassList.R', echo=TRUE)
names(GBIFExtClassList)
names(GBIFCoreClassList)
source('~/Work/LivingNorway/LivingNorwayR_GitHub/data-raw/DwCClassList.R', echo=TRUE)
source('~/Work/LivingNorway/LivingNorwayR_GitHub/data-raw/createClassInfrastructure.R', echo=TRUE)
createGBIFClassInfrastructure("C:/Users/joseph.chipperfield/OneDrive - NINA/Work/LivingNorway/LivingNorwayR_GitHub/R")
stop("hello", "joe")
localeToCharset(Sys.getlocale("LC_CTYPE"))
?unzip
unzip("ssngis")
